{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Load MinIO credentials\n",
    "try:\n",
    "    creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "    print(\"MinIO credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"MinIO credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load MySQL credentials\n",
    "try:\n",
    "    db_creds_file = open(f\"/home/{os.getenv('USER')}/database-creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    dbusername, dbpassword = db_creds_file[0], db_creds_file[1]\n",
    "    print(\"Database credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Spark configuration\n",
    "conf = SparkConf() \\\n",
    "    .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,org.apache.hadoop:hadoop-common:3.2.3,mysql:mysql-connector-java:8.0.33') \\\n",
    "    .set('spark.hadoop.fs.s3a.access.key', accesskey) \\\n",
    "    .set('spark.hadoop.fs.s3a.secret.key', secretkey) \\\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .set('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\") \\\n",
    "    .setMaster(\"spark://sm.service.consul:7077\") \\\n",
    "    .set(\"spark.driver.memory\", \"8g\") \\\n",
    "    .set(\"spark.executor.memory\", \"4g\") \\\n",
    "    .set(\"spark.cores.max\", \"10\") \\\n",
    "    .set(\"spark.executor.cores\", \"1\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"gchoi6-module-11-part1\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read and process the cleaned parquet file\n",
    "try:\n",
    "    df = spark.read.parquet(\"s3a://itmd521/50-cleaned.parquet\")\n",
    "    print(\"Parquet file loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to read parquet file from MinIO: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Save DataFrame to MySQL\n",
    "try:\n",
    "    mysql_url = f\"jdbc:mysql://system75.rice.iit.edu:3306/gchoi6\"\n",
    "    mysql_table = \"d50\"\n",
    "\n",
    "    df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", mysql_url) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"dbtable\", mysql_table) \\\n",
    "        .option(\"user\", dbusername) \\\n",
    "        .option(\"password\", dbpassword) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(\"Data successfully written to MySQL.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to write data to MySQL: {e}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test if data is loading properly from MinIO\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Load MinIO credentials\n",
    "try:\n",
    "    creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "    print(\"MinIO credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"MinIO credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Stop any existing SparkContext or SparkSession\n",
    "if SparkContext._active_spark_context is not None:\n",
    "    print(\"Stopping existing SparkContext...\")\n",
    "    SparkContext._active_spark_context.stop()\n",
    "    print(\"Existing SparkContext stopped successfully.\")\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,org.apache.hadoop:hadoop-common:3.2.3') \\\n",
    "    .set('spark.hadoop.fs.s3a.access.key', accesskey) \\\n",
    "    .set('spark.hadoop.fs.s3a.secret.key', secretkey) \\\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .set('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .setAppName(\"Data Count Example\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Load and count data from s3a://itmd521/50-cleaned.parquet\n",
    "try:\n",
    "    df_50 = spark.read.parquet(\"s3a://itmd521/50-cleaned.parquet\")\n",
    "    print(\"Data from 50-cleaned.parquet loaded successfully!\")\n",
    "    count_50 = df_50.count()  # Count total records\n",
    "    print(f\"Total records in df_50: {count_50}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from 50-cleaned.parquet: {e}\")\n",
    "\n",
    "# Load and count data from s3a://itmd521/60-cleaned.parquet\n",
    "try:\n",
    "    df_60 = spark.read.parquet(\"s3a://itmd521/60-cleaned.parquet\")\n",
    "    print(\"Data from 60-cleaned.parquet loaded successfully!\")\n",
    "    count_60 = df_60.count()  # Count total records\n",
    "    print(f\"Total records in df_60: {count_60}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from 60-cleaned.parquet: {e}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert to JSON format since 60.json does not exist and is required for the lab\n",
    "# Save the JSON format under my username which is gchoi6\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Load MinIO credentials\n",
    "try:\n",
    "    creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "    print(\"MinIO credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"MinIO credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Stop any existing SparkContext or SparkSession\n",
    "if SparkContext._active_spark_context is not None:\n",
    "    print(\"Stopping existing SparkContext...\")\n",
    "    SparkContext._active_spark_context.stop()\n",
    "    print(\"Existing SparkContext stopped successfully.\")\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,org.apache.hadoop:hadoop-common:3.2.3') \\\n",
    "    .set('spark.hadoop.fs.s3a.access.key', accesskey) \\\n",
    "    .set('spark.hadoop.fs.s3a.secret.key', secretkey) \\\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .set('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .setAppName(\"Parquet to JSON and Merge Example\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Step 1: Read Parquet file and write to JSON\n",
    "try:\n",
    "    df_60 = spark.read.parquet(\"s3a://itmd521/60-cleaned.parquet\")\n",
    "    print(\"Data from 60-cleaned.parquet loaded successfully!\")\n",
    "    output_path = \"s3a://gchoi6/output/60.json\"\n",
    "    df_60.write.json(output_path, mode=\"overwrite\")\n",
    "    print(f\"60-cleaned.parquet successfully written to {output_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing Parquet to JSON conversion: {e}\")\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Now we have two datasets: 60.json compressed from Minio and the 50 JDBC table.\n",
    "# Load both 50 JTBC table and 60.json file as dataframe (df_50 and df_60)\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, stddev, count, expr, lit\n",
    "import os\n",
    "\n",
    "# Load MinIO credentials\n",
    "try:\n",
    "    creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "    print(\"MinIO credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"MinIO credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Load MySQL credentials\n",
    "try:\n",
    "    db_creds_file = open(f\"/home/{os.getenv('USER')}/database-creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    dbusername, dbpassword = db_creds_file[0], db_creds_file[1]\n",
    "    print(\"Database credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Stop any existing SparkContext or SparkSession\n",
    "if SparkContext._active_spark_context is not None:\n",
    "    print(\"Stopping existing SparkContext...\")\n",
    "    SparkContext._active_spark_context.stop()\n",
    "    print(\"Existing SparkContext stopped successfully.\")\n",
    "\n",
    "# Spark Configuration\n",
    "conf = SparkConf() \\\n",
    "    .set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,org.apache.hadoop:hadoop-common:3.2.3,mysql:mysql-connector-java:8.0.33') \\\n",
    "    .set('spark.hadoop.fs.s3a.access.key', accesskey) \\\n",
    "    .set('spark.hadoop.fs.s3a.secret.key', secretkey) \\\n",
    "    .set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .set('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .set('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .set(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\") \\\n",
    "    .setMaster(\"local[*]\") \\\n",
    "    .setAppName(\"Lab11 Part 2\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "try:\n",
    "    df_60 = spark.read.json(\"s3a://gchoi6/output/60.json\")\n",
    "    print(\"60.json loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading 60.json: {e}\")\n",
    "\n",
    "try:\n",
    "    jdbc_url = f\"jdbc:mysql://system75.rice.iit.edu:3306/gchoi6\"\n",
    "    df_50 = spark.read \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", jdbc_url) \\\n",
    "        .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "        .option(\"dbtable\", \"d50\") \\\n",
    "        .option(\"user\", dbusername) \\\n",
    "        .option(\"password\", dbpassword) \\\n",
    "        .load()\n",
    "    print(\"d50 table loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading d50 table: {e}\")\n",
    "\n",
    "# Combine the dataframesa\n",
    "combined_df = df_50.union(df_60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment and try this if above df_50 from MySQL database is not loading preoperly\n",
    "\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import month, year, avg, stddev, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import os\n",
    "\n",
    "# Load MinIO credentials\n",
    "try:\n",
    "    creds_file = open(f\"/home/{os.getenv('USER')}/creds.txt\", \"r\").read().strip().split(\",\")\n",
    "    accesskey, secretkey = creds_file[0], creds_file[1]\n",
    "    print(\"MinIO credentials loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"MinIO credentials file not found. Exiting.\")\n",
    "    exit(1)\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Weather Data Analytics\") \\\n",
    "    .config('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.2.3,org.apache.hadoop:hadoop-common:3.2.3') \\\n",
    "    .config('spark.hadoop.fs.s3a.access.key', accesskey) \\\n",
    "    .config('spark.hadoop.fs.s3a.secret.key', secretkey) \\\n",
    "    .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "    .config('spark.hadoop.fs.s3a.path.style.access', 'true') \\\n",
    "    .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://system54.rice.iit.edu\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load df_50 and df_60\n",
    "try:\n",
    "    df_50 = spark.read.parquet(\"s3a://itmd521/50-cleaned.parquet\")\n",
    "    df_60 = spark.read.parquet(\"s3a://itmd521/60-cleaned.parquet\")\n",
    "    print(\"Data from both parquet files loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data from parquet files: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Combine the dataframes\n",
    "combined_df = df_50.union(df_60)\n",
    "\"\"\"\n",
    "\n",
    "# Filter for February data and remove unrealistic values\n",
    "february_df = combined_df.filter(\n",
    "    (month(combined_df.ObservationDate) == 2) &\n",
    "    (combined_df.AirTemperature > -100) &\n",
    "    (combined_df.AirTemperature < 100) &\n",
    "    (combined_df.AtmosphericPressure > 800) &\n",
    "    (combined_df.AtmosphericPressure < 1100)\n",
    ")\n",
    "\n",
    "# Create a temporary view for SQL queries\n",
    "february_df.createOrReplaceTempView(\"february_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Count the number of records total\n",
    "total_count = february_df.count()\n",
    "count_df = spark.createDataFrame([(total_count,)], [\"TotalCount\"])\n",
    "count_df.coalesce(1).write.csv(\"february_count.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 2. Average air temperature for month of February for each year\n",
    "avg_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, AVG(AirTemperature) as AvgTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "avg_temp.coalesce(1).write.csv(\"february_avg_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 3. Median air temperature for month of February for each year\n",
    "median_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, \n",
    "           percentile_approx(AirTemperature, 0.5) as MedianTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "median_temp.coalesce(1).write.csv(\"february_median_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 4. Standard Deviation of air temperature for month of February for each year\n",
    "stddev_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, STDDEV(AirTemperature) as StdDevTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "stddev_temp.coalesce(1).write.csv(\"february_stddev_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 5. AVG air temperature per StationID in the month of February for each year\n",
    "avg_temp_station = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, WeatherStation, AVG(AirTemperature) as AvgTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate), WeatherStation\n",
    "    ORDER BY Year, WeatherStation\n",
    "\"\"\")\n",
    "avg_temp_station.coalesce(1).write.csv(\"february_avg_temp_by_station.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# Display results\n",
    "print(f\"Total count of February records: {total_count}\")\n",
    "print(\"\\nAverage air temperature for February by year:\")\n",
    "avg_temp.show()\n",
    "print(\"\\nMedian air temperature for February by year:\")\n",
    "median_temp.show()\n",
    "print(\"\\nStandard deviation of air temperature for February by year:\")\n",
    "stddev_temp.show()\n",
    "print(\"\\nAverage air temperature per station for February by year (first 20 rows):\")\n",
    "avg_temp_station.show(20)\n",
    "\n",
    "# Uncomment this if you want to produce csv files in MinIO instead of local directory.\n",
    "# I also ran below code, so you can see my final_lab csv files under gchoi6 folder in MinIO\n",
    "\"\"\"\n",
    "# 1. Count the number of records total\n",
    "total_count = february_df.count()\n",
    "count_df = spark.createDataFrame([(total_count,)], [\"TotalCount\"])\n",
    "count_df.coalesce(1).write.csv(\"s3a://gchoi6/final_lab_output/february_count.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 2. Average air temperature for month of February for each year\n",
    "avg_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, AVG(AirTemperature) as AvgTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "avg_temp.coalesce(1).write.csv(\"s3a://gchoi6/final_lab_output/february_avg_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 3. Median air temperature for month of February for each year\n",
    "median_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, \n",
    "           percentile_approx(AirTemperature, 0.5) as MedianTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "median_temp.coalesce(1).write.csv(\"s3a://gchoi6/final_lab_output/february_median_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 4. Standard Deviation of air temperature for month of February for each year\n",
    "stddev_temp = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, STDDEV(AirTemperature) as StdDevTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate)\n",
    "    ORDER BY Year\n",
    "\"\"\")\n",
    "stddev_temp.coalesce(1).write.csv(\"s3a://gchoi6/final_lab_output/february_stddev_temp.csv\", header=True, mode=\"overwrite\")\n",
    "\n",
    "# 5. AVG air temperature per StationID in the month of February for each year\n",
    "avg_temp_station = spark.sql(\"\"\"\n",
    "    SELECT YEAR(ObservationDate) as Year, WeatherStation, AVG(AirTemperature) as AvgTemperature\n",
    "    FROM february_data\n",
    "    GROUP BY YEAR(ObservationDate), WeatherStation\n",
    "    ORDER BY Year, WeatherStation\n",
    "\"\"\")\n",
    "avg_temp_station.coalesce(1).write.csv(\"s3a://gchoi6/final_lab_output/february_avg_temp_by_station.csv\", header=True, mode=\"overwrite\")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
